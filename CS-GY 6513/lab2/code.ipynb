{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb533b3-4425-47b6-815b-b31722be070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 23:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-rl5083:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rl5083-hw2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=rl5083-hw2>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"rl5083-hw2\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faf9197-8495-468b-8aca-9d8d8fc665be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x78811512ced0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = pyspark.SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6c77f-360c-4b3e-b584-7935f77d30a7",
   "metadata": {},
   "source": [
    "# Question 1 - 25 pts\n",
    "\n",
    "Data: shared/data/Bakery.csv   \n",
    "\n",
    "Show the highest selling item for Mondays, per hour, for the 7AM to 11AM hours. Note that \"weekday\", \"period\" have to be computed.\n",
    "\n",
    "For example (these are made up numbers...)    \n",
    " Item qty, weekday, Date , Hour-period, qty    \n",
    " Bread, 102, Monday, 2016-10-31, 7AM   \n",
    " Coffee, 132, Monday, 2016-10-31, 8AM    \n",
    " :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d92a34-029b-45ec-bb9a-ce74a730b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, hour, count, when, dayofweek\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4744a39-58dc-4ae4-a06d-f9d1171caf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 23:41:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Selling Items for Mondays (7AM to 11AM):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+----------+-----------+\n",
      "|     Item|qty|weekday|      Date|Hour-period|\n",
      "+---------+---+-------+----------+-----------+\n",
      "|   Coffee|  2| Monday|2016-10-31|        8AM|\n",
      "|   Coffee| 11| Monday|2016-10-31|        9AM|\n",
      "|   Coffee| 10| Monday|2016-10-31|       10AM|\n",
      "|   Coffee| 13| Monday|2016-10-31|       11AM|\n",
      "|   Coffee|  1| Monday|2016-11-07|        8AM|\n",
      "|   Pastry|  3| Monday|2016-11-07|        9AM|\n",
      "|   Coffee|  7| Monday|2016-11-07|       10AM|\n",
      "|   Coffee| 10| Monday|2016-11-07|       11AM|\n",
      "|   Coffee|  1| Monday|2016-11-14|        7AM|\n",
      "|   Coffee|  2| Monday|2016-11-14|        8AM|\n",
      "|   Coffee|  5| Monday|2016-11-14|        9AM|\n",
      "|   Coffee|  5| Monday|2016-11-14|       10AM|\n",
      "|    Bread|  5| Monday|2016-11-14|       11AM|\n",
      "|   Coffee|  1| Monday|2016-11-21|        7AM|\n",
      "|   Coffee|  2| Monday|2016-11-21|        8AM|\n",
      "|   Coffee|  8| Monday|2016-11-21|        9AM|\n",
      "|   Coffee|  4| Monday|2016-11-21|       10AM|\n",
      "|   Coffee|  4| Monday|2016-11-21|       11AM|\n",
      "|   Coffee|  1| Monday|2016-11-28|        7AM|\n",
      "|Medialuna|  1| Monday|2016-11-28|        8AM|\n",
      "+---------+---+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"shared/data/Bakery.csv\") \n",
    "\n",
    "df = df.withColumn(\"Hour\", date_format(col(\"Time\"), \"hha\"))\n",
    "df = df.withColumn(\"weekday\", \n",
    "    when(date_format(col(\"Date\"), \"E\") == \"Mon\", \"Monday\")\n",
    "    .when(date_format(col(\"Date\"), \"E\") == \"Tue\", \"Tuesday\")\n",
    "    .when(date_format(col(\"Date\"), \"E\") == \"Wed\", \"Wednesday\")\n",
    "    .when(date_format(col(\"Date\"), \"E\") == \"Thu\", \"Thursday\")\n",
    "    .when(date_format(col(\"Date\"), \"E\") == \"Fri\", \"Friday\")\n",
    "    .when(date_format(col(\"Date\"), \"E\") == \"Sat\", \"Saturday\")\n",
    "    .otherwise(\"Sunday\")  # Assuming 7 represents Sunday\n",
    ")\n",
    "df = df.filter((col(\"weekday\") == \"Monday\") & (col(\"Hour\").between(\"07AM\", \"11AM\")))\n",
    "\n",
    "sales_per_hour = df.groupBy(\"Date\", \"Hour\", \"Item\").agg(count(\"Transaction\").alias(\"qty\"))\n",
    "\n",
    "highest_selling_items = sales_per_hour.withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"Date\", \"Hour\").orderBy(col(\"qty\").desc()))).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "highest_selling_items = highest_selling_items.withColumn(\"weekday\", F.date_format(col(\"Date\"), \"EEEE\"))\n",
    "highest_selling_items = highest_selling_items.withColumn(\"Hour\", F.regexp_replace(col(\"Hour\"), r\"^0\", \"\"))\n",
    "highest_selling_items = highest_selling_items.select(\"Item\", \"qty\", \"weekday\", \"Date\", F.col(\"Hour\").alias(\"Hour-period\"))\n",
    "\n",
    "print(\"Highest Selling Items for Mondays (7AM to 11AM):\")\n",
    "highest_selling_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bad27a-aa78-4f4f-9b6d-36c536a3d0cb",
   "metadata": {},
   "source": [
    "# Question 2 - 25 pts\n",
    "\n",
    "Data: shared/data/Bakery.csv\n",
    "\n",
    "Show the top 2 (by qty) items bought by Daypart, by DayType.\n",
    "\n",
    "Note:    \n",
    "Daypart = Breakfast if 6AM – 10:59AM, Lunch if 11:01AM – 3:59PM, Dinner   \n",
    "otherwise   \n",
    "DayType = Weekend if Sat, Sun, Weekday otherwise\n",
    "\n",
    "For example (not necessarily the right numbers….)     \n",
    " Weekend, Breakfast, (coffee, Muffin)   \n",
    " Weekend, Lunch, (cookies, pastry)   \n",
    "\n",
    "The Answer MUST include the 2 items in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5959c45c-f482-47c3-ace4-15efdfaaf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, concat_ws, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21679f1-e0e6-407a-a923-287c19a903ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 (by qty) items bought by Daypart, by DayType:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+\n",
      "|DayType|  Daypart|  Top-2 Items|\n",
      "+-------+---------+-------------+\n",
      "|Weekday|Breakfast|Coffee, Bread|\n",
      "|Weekday|   Dinner|Coffee, Bread|\n",
      "|Weekday|    Lunch|Coffee, Bread|\n",
      "|Weekend|Breakfast|Coffee, Bread|\n",
      "|Weekend|   Dinner|Coffee, Bread|\n",
      "|Weekend|    Lunch|Coffee, Bread|\n",
      "+-------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"shared/data/Bakery.csv\") \n",
    "\n",
    "df2 = df2.withColumn(\"DayType\",\n",
    "                   when(dayofweek(col(\"Date\")) == 7, \"Weekend\")  # Saturday\n",
    "                   .when(dayofweek(col(\"Date\")) == 1, \"Weekend\")  # Sunday\n",
    "                   .otherwise(\"Weekday\"))\n",
    "df2 = df2.withColumn(\"Daypart\",\n",
    "                   when((col(\"Time\").between(\"06:00:00\", \"10:59:59\")), \"Breakfast\")\n",
    "                   .when((col(\"Time\").between(\"11:00:00\", \"15:59:59\")), \"Lunch\")\n",
    "                   .otherwise(\"Dinner\"))\n",
    "\n",
    "item_counts = df2.groupBy(\"DayType\", \"Daypart\", \"Item\").agg(count(\"Transaction\").alias(\"qty\"))\n",
    "\n",
    "top_items = item_counts.withColumn(\"rank\", rank().over(Window.partitionBy(\"DayType\", \"Daypart\").orderBy(col(\"qty\").desc()))).filter(col(\"rank\") <= 2)\n",
    "top_items = top_items.groupBy(\"DayType\", \"Daypart\").agg(concat_ws(\", \", collect_list(\"Item\")).alias(\"Top-2 Items\"))\n",
    "\n",
    "print(\"Top-2 (by qty) items bought by Daypart, by DayType:\")\n",
    "top_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a17285-253e-4f16-acaa-1855cbe74616",
   "metadata": {},
   "source": [
    "# Question 3 - 20 pts\n",
    "\n",
    "Data: shared/data/Restaurants_in_Durham_County_NC.json\n",
    "\n",
    "Show the number of entities by \"fields.rpt_area_desc\"\n",
    "\n",
    "Example (not true numbers):     \n",
    " \"Food Service\", 13   \n",
    " \"Tatoo Establishment\", 2   \n",
    " :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355db455-f0e0-468d-ab00-43fc37cb759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the number of entities by 'fields.rpt_area_desc':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       rpt_area_desc|count|\n",
      "+--------------------+-----+\n",
      "|  Bed&Breakfast Home|    4|\n",
      "|        Summer Camps|    4|\n",
      "|        Institutions|   30|\n",
      "|   Local Confinement|    2|\n",
      "|         Mobile Food|  147|\n",
      "|    School Buildings|   89|\n",
      "|         Summer Food|  242|\n",
      "|      Swimming Pools|  420|\n",
      "|            Day Care|  173|\n",
      "|Tattoo Establishm...|   36|\n",
      "|    Residential Care|  154|\n",
      "|   Bed&Breakfast Inn|    2|\n",
      "|      Adult Day Care|    5|\n",
      "|             Lodging|   62|\n",
      "|        Food Service| 1093|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.json(\"shared/data//Restaurants_in_Durham_County_NC.json\")\n",
    "\n",
    "df3 = df3.groupBy(\"fields.rpt_area_desc\").count()\n",
    "\n",
    "print(\"Show the number of entities by \\'fields.rpt_area_desc\\':\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60631715-6b55-40e5-a8e4-767408924445",
   "metadata": {},
   "source": [
    "# Question 4 - 20 pts\n",
    "\n",
    "Data: shared/data/populationbycountry19802010millions.csv\n",
    "\n",
    "Show the country or region with the biggest percentage increase in population AND the country with biggest percentage decrease in population, between the years 1990 and 2000. Use only the countries, not 'World'.\n",
    "\n",
    "Example (Not the real answer):    \n",
    "North America, 2.30% <- assuming North America was max    \n",
    "Aruba, -22.2%… <- assuming Aruba was min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ab1950-0ca8-4cd2-ae1d-6addbabd512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfb1cff-d70c-43db-adc9-f2a0d365ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 23:41:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest percentage increase in population between 1990 and 2000:\n",
      "+--------------------+------------------+\n",
      "|             Country|            Change|\n",
      "+--------------------+------------------+\n",
      "|United Arab Emirates|76.27926078028749%|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 23:41:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
      " Schema: _c0, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/data/populationbycountry19802010millions.csv\n"
     ]
    }
   ],
   "source": [
    "df4 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"shared/data/populationbycountry19802010millions.csv\") \n",
    "\n",
    "df4= df4.withColumnRenamed(df4.columns[0], \"Country\")\n",
    "df4 = df4.filter(col(\"Country\") != \"World\")\n",
    "df4 = df4.withColumn(\"Change\", ((col(\"2000\") - col(\"1990\")) / col(\"1990\")) * 100)\n",
    "df4 = df4.na.drop()\n",
    "df4.createOrReplaceTempView(\"population_data\")\n",
    "\n",
    "max_increase_result = spark.sql(\"\"\"\n",
    "SELECT Country, Change \n",
    "FROM population_data\n",
    "ORDER BY Change DESC \n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "max_increase_result = max_increase_result.withColumn(\"Change\", concat(col(\"Change\"), lit(\"%\")))\n",
    "\n",
    "print(\"The biggest percentage increase in population between 1990 and 2000:\")\n",
    "max_increase_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "465f7ae2-5429-4a5e-917a-02e08e367e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest percentage decrease in population between 1990 and 2000:\n",
      "+----------+-------------------+\n",
      "|   Country|             Change|\n",
      "+----------+-------------------+\n",
      "|Montserrat|-63.18732525629077%|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 23:41:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
      " Schema: _c0, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/data/populationbycountry19802010millions.csv\n"
     ]
    }
   ],
   "source": [
    "max_decrease_result = spark.sql(\"\"\"\n",
    "SELECT Country, Change\n",
    "FROM population_data\n",
    "ORDER BY Change\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "max_decrease_result = max_decrease_result.withColumn(\"Change\", concat(col(\"Change\"), lit(\"%\")))\n",
    "\n",
    "print(\"The biggest percentage decrease in population between 1990 and 2000:\")\n",
    "max_decrease_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93c1d7-b894-468c-8f36-e64dafce799f",
   "metadata": {},
   "source": [
    "# Question 5 - 20 pts\n",
    "\n",
    "Data: hw1text (from HW1). \n",
    "\n",
    "Solve: do WordCount\n",
    "\n",
    "Do word count exercise using pyspark. Ignore punctuation and normalize to lower case. i.e. replace characters in NOT in this set: [0-9a-z] with space.\n",
    "\n",
    "HINT: You can use the sparkml package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f367ef24-38a8-45c9-804e-8dedaeb66039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.sql.functions import lower, regexp_replace, explode, split\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50860b3-f5d8-43d1-ba4f-eb8d4936e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we put hw1text.zip and the Notebook under the same folder\n",
    "\n",
    "zip_file_path = 'hw1text.zip'\n",
    "extraction_dir = 'hw1text/'\n",
    "\n",
    "os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n",
    "\n",
    "extracted_files = os.listdir(extraction_dir)\n",
    "\n",
    "text_files_path = [\n",
    "    \"20-01.txt\",\n",
    "    \"20-02.txt\",\n",
    "    \"20-03.txt\",\n",
    "    \"20-04.txt\",\n",
    "    \"20-05.txt\"\n",
    "]\n",
    "\n",
    "text_files = [os.path.join(extraction_dir, file) for file in text_files_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087b5749-e4ed-45de-9e04-2da1fc25a15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "| the|163547|\n",
      "|  to| 89046|\n",
      "|   p| 78664|\n",
      "|  of| 75568|\n",
      "| and| 72730|\n",
      "|  in| 56782|\n",
      "|   a| 53198|\n",
      "| for| 29770|\n",
      "|that| 28852|\n",
      "|  is| 27601|\n",
      "|  on| 24485|\n",
      "|   s| 23615|\n",
      "|with| 19575|\n",
      "| are| 19417|\n",
      "|  it| 18231|\n",
      "|  be| 17998|\n",
      "|  as| 17796|\n",
      "|have| 16188|\n",
      "|  at| 15965|\n",
      "|  we| 15754|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df5 = spark.read.text(text_files)\n",
    "\n",
    "df5 = df5.select(\n",
    "    regexp_replace(lower(col(\"value\")), \"[^0-9a-z]+\", \" \").alias(\"cleaned_text\")\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df5)\n",
    "word_counts = words_data.select(explode(col(\"words\")).alias(\"word\")).groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "print(\"Word Count:\")\n",
    "word_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07333e8-d3c5-498a-b287-7c7d3606a626",
   "metadata": {},
   "source": [
    "# Question 6 - 20 pts\n",
    "\n",
    "Data: hw1text (from HW1)\n",
    "\n",
    "Find the 10 most common bigrams.\n",
    "\n",
    "HINT: You can use the sparkml package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1425492-11f6-4bd6-a3e2-b761700592e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d495907-2925-45b2-bbcf-78def894778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 most common bigrams:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  bigram|count|\n",
      "+--------+-----+\n",
      "|  of the|17484|\n",
      "|  in the|12808|\n",
      "|   p the|10363|\n",
      "|covid 19| 8762|\n",
      "|  to the| 8372|\n",
      "| for the| 5588|\n",
      "|     n t| 5393|\n",
      "|  on the| 5032|\n",
      "|   to be| 4581|\n",
      "| will be| 4177|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df6 = spark.read.text(text_files)\n",
    "\n",
    "df6 = df6.select(\n",
    "    regexp_replace(lower(col(\"value\")), \"[^0-9a-z]+\", \" \").alias(\"cleaned_text\")\n",
    ")\n",
    "\n",
    "tokenizer6 = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "words_data6 = tokenizer6.transform(df6)\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigram_data = ngram.transform(words_data6)\n",
    "bigram_counts = bigram_data.select(explode(col(\"bigrams\")).alias(\"bigram\")).groupBy(\"bigram\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "\n",
    "print(\"Top-10 most common bigrams:\")\n",
    "bigram_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7b93c-b593-4546-bbe9-b1bd5e4dcb10",
   "metadata": {},
   "source": [
    "# Question 7 (Extra credit) - 40 pts\n",
    "\n",
    "Data: durham-nc-foreclosure-2006-2016.json, Restaurants_in_Durham_County_NC.json\n",
    " \n",
    "a) Find food service and active restaurants (\"status\" = \"ACTIVE\" and \"rpt_area_desc\" = \"Food Service\") closest to the following coordinate: of 35.994914, -78.897133, and show it.\n",
    "\n",
    "b) With that restaurant in (a) as your center point, find the number of foreclosures within a 1 mile radius.\n",
    "\n",
    "You can use an external library for calculating coordinate distances. The haversine library is available in Jupyterhub’s bigdata environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d34e3-b07b-445e-b16b-2b862eff9a24",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c492b39-be09-421b-aa27-74968f51af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031aaab3-5fbc-4c43-833f-c839886fea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7a = spark.read.json(\"shared/data/Restaurants_in_Durham_County_NC.json\")\n",
    "df7a.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102a26f5-8177-43ef-b81c-59eb77b95945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(row):\n",
    "    lat = row['fields']['geolocation'][0]\n",
    "    lon = row['fields']['geolocation'][1]\n",
    "    return (row['fields']['premise_name'], lat, lon, haversine(center_point, (lat, lon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8611bfc6-edb1-4b56-a652-6aaa9425650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"restaurant_name\", StringType(), True),\n",
    "    StructField(\"lat\", FloatType(), True),\n",
    "    StructField(\"lon\", FloatType(), True),\n",
    "    StructField(\"distance\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58105d1f-345f-41e0-89ec-02e09a03b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- restaurant_name: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- distance: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "center_point = (35.994914, -78.897133)\n",
    "\n",
    "df7a = df7a.filter(\n",
    "    (F.col(\"fields.status\") == \"ACTIVE\") &\n",
    "    (F.col(\"fields.rpt_area_desc\") == \"Food Service\") &\n",
    "    (F.col(\"fields.geolocation\").isNotNull())\n",
    ")\n",
    "df7a = df7a.rdd.map(calculate_distance)\n",
    "df7a = spark.createDataFrame(df7a, schema)\n",
    "df7a.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c943e86-f447-4802-8fd4-f0f2beaf1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The restaurant which is closest to (35.994914, -78.897133):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+---------+\n",
      "|     restaurant_name|      lat|      lon| distance|\n",
      "+--------------------+---------+---------+---------+\n",
      "|OLD HAVANA SANDWI...|35.993282|-78.89813|0.2024912|\n",
      "+--------------------+---------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df7a = df7a.orderBy(\"distance\").limit(1)\n",
    "\n",
    "print(\"The restaurant which is closest to (35.994914, -78.897133):\")\n",
    "df7a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751874f-67a5-423a-92c1-b90c6a921e32",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f8424ed-f6a0-4dd8-8461-419fc313027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- geocode: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- parcel_number: string (nullable = true)\n",
      " |    |-- year: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7b = spark.read.json(\"shared/data/durham-nc-foreclosure-2006-2016.json\")\n",
    "schema = df7b.schema\n",
    "df7b.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb2dc67-a445-4206-8e28-2b8ba021490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "coordinates = df7a.select(\"lat\", \"lon\").first()\n",
    "center_lat = coordinates[\"lat\"]\n",
    "center_lon = coordinates[\"lon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9579d4-7d30-4d3a-8e8b-bb47895e2e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center point: (35.993282318115234, -78.89813232421875)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Center point: ({center_lat}, {center_lon})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "935c71a0-3447-4d80-a88d-9c0a982a6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreclosure_within_radius(row):\n",
    "    if row['fields']['geocode']:\n",
    "        lat = row['fields']['geocode'][0]\n",
    "        lon = row['fields']['geocode'][1]\n",
    "        distance = haversine((center_lat, center_lon), (lat, lon), unit=Unit.MILES)\n",
    "        return distance <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34b8f743-cafb-42bb-96a1-6cafd9b1c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of foreclosures within a 1 mile radius: 320\n"
     ]
    }
   ],
   "source": [
    "foreclosures_within_radius = df7b.rdd.filter(foreclosure_within_radius)\n",
    "df7b = spark.createDataFrame(foreclosures_within_radius, schema)\n",
    "print(\"The number of foreclosures within a 1 mile radius:\", df7b.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf785a7-7642-44bd-a98f-93d0ee73cb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
