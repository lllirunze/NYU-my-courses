{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5085f6-91ed-4225-a569-593e1ed02021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/09 02:18:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-rl5083:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rl5083-midterm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=rl5083-midterm>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"rl5083-midterm\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4347fe-33cf-4311-9ae8-9c8702303de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bigdata-spark/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x78dbfda0ff10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/09 02:18:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ea247-9e2e-4b86-86ba-c0af623de801",
   "metadata": {},
   "source": [
    "# Question 2: Language Models - 50 points\n",
    "\n",
    "Discussion: If you have bigrams and trigrams, you can implement a simple Language Model that predicts the third word in a three-word sequence.\n",
    "\n",
    "For example, to compute the probability P(times/new york), that is 'times' following the word 'new york', we use Maximum Likelihood Estime, i.e. MLE (ignoring out of vocabulary words, i.e. no smoothing)\n",
    "\n",
    "Use MLE as follows:\n",
    "\n",
    "$P(times | new~york) = \\frac{count(trigrams(new~york~times))}{count(bigrams(new~york))}$\n",
    "\n",
    "The input here is the same text input as homeworks 1 and 2: hw1text/*.\n",
    "\n",
    "TO-DO: Compute the top 10 trigrams (count) for the input text, and show the conditional probability of the third word of each trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a50812-f32c-4b18-896d-fb57328383ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import col, count, regexp_replace, lower, explode, split, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59f58e2-8e1c-4c1b-a8fc-0d60bf9f612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = 'hw1text.zip'\n",
    "extraction_dir = 'hw1text/'\n",
    "\n",
    "os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n",
    "\n",
    "extracted_files = os.listdir(extraction_dir)\n",
    "\n",
    "text_files_path = [\n",
    "    \"20-01.txt\",\n",
    "    \"20-02.txt\",\n",
    "    \"20-03.txt\",\n",
    "    \"20-04.txt\",\n",
    "    \"20-05.txt\"\n",
    "]\n",
    "\n",
    "text_files = [os.path.join(extraction_dir, file) for file in text_files_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3024f579-82cd-448c-9db3-fd69283fd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.text(text_files)\n",
    "df2_cleaned = df2.select(regexp_replace(lower(col(\"value\")), \"[^0-9a-z]+\", \" \").alias(\"cleaned_text\"))\n",
    "df2 = df2_cleaned.select(split(col(\"cleaned_text\"), \" \").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb841fb1-a34f-4bf6-bd7e-840161a3b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigrams = ngram.transform(df2)\n",
    "bigram_counts = bigrams.select(explode(col(\"bigrams\")).alias(\"bigram\")).groupBy(\"bigram\").agg(count(\"*\").alias(\"bigram_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9e11ae-404c-41ec-a30a-74601876b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "trigrams = ngram.transform(df2)\n",
    "trigram_counts = trigrams.select(explode(col(\"trigrams\")).alias(\"trigram\")).groupBy(\"trigram\").agg(count(\"*\").alias(\"trigram_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0534c39-b935-4595-a6cb-d5e1c8dd5403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+----------+------------+-----------------------+\n",
      "|trigram           |trigram_count|bigram    |bigram_count|conditional_probability|\n",
      "+------------------+-------------+----------+------------+-----------------------+\n",
      "|lt p gt           |1928         |lt p      |1930        |0.9989637305699481     |\n",
      "|the covid 19      |1718         |the covid |1760        |0.9761363636363637     |\n",
      "|do n t            |1662         |do n      |1662        |1.0                    |\n",
      "|of covid 19       |1589         |of covid  |1621        |0.9802590993214065     |\n",
      "|the spread of     |1196         |the spread|1306        |0.9157733537519143     |\n",
      "|p gt lt           |1037         |p gt      |1936        |0.5356404958677686     |\n",
      "|the number of     |1037         |the number|1103        |0.9401631912964642     |\n",
      "|gt lt p           |1023         |gt lt     |1792        |0.5708705357142857     |\n",
      "|one of the        |953          |one of    |1491        |0.6391683433936955     |\n",
      "|of the coronavirus|907          |of the    |17484       |0.0518760009151224     |\n",
      "+------------------+-------------+----------+------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trigram_counts = trigram_counts.withColumn(\"bigram\", split(col(\"trigram\"), \" \").getItem(0)).withColumn(\"bigram\", concat_ws(\" \", col(\"bigram\"), split(col(\"trigram\"), \" \").getItem(1)))\n",
    "\n",
    "conditional_probabilities = trigram_counts.join(bigram_counts, \"bigram\").withColumn(\"conditional_probability\", col(\"trigram_count\") / col(\"bigram_count\")).select(\"trigram\", \"trigram_count\", \"bigram\", \"bigram_count\", \"conditional_probability\").orderBy(col(\"trigram_count\").desc()).limit(10)\n",
    "conditional_probabilities.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b012a-2557-48ba-861e-031462d571c8",
   "metadata": {},
   "source": [
    "# Question 3: Ranking over Partitions - 50 points\n",
    "\n",
    "Compute the top 3 items sold per daypart for the problem in Homework 2, Question 1. (Bakery.csv)\n",
    "\n",
    "Daypart = \n",
    "- morning if time >= 6AM, < 11AM\n",
    "- noon if time >= 11AM, < 2PM\n",
    "- afternoon if time >= 2PM, < 5PM\n",
    "- evening if time >= 5PM, <6AM \n",
    "\n",
    "The format MUST be in the format (example) and show ALL rows:     \n",
    "morning coffe, bagel, pastry      \n",
    "noon soda, ham, potatoes   \n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9effc61a-d5c0-487f-9655-0b0439366932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, rank, collect_list\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53575bcd-187f-42a2-8169-e9e100938d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 (by qty) items bought by Daypart:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  Daypart|         Top-3 Items|\n",
      "+---------+--------------------+\n",
      "|afternoon|  Coffee, Bread, Tea|\n",
      "|  evening|  Coffee, Bread, Tea|\n",
      "|  morning|Coffee, Bread, Pa...|\n",
      "|     noon|  Coffee, Bread, Tea|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"shared/data/Bakery.csv\") \n",
    "df3 = df3.withColumn(\"Daypart\",\n",
    "                when((col(\"Time\").between(\"06:00:00\", \"10:59:59\")), \"morning\")\n",
    "                .when((col(\"Time\").between(\"11:00:00\", \"13:59:59\")), \"noon\")\n",
    "                .when((col(\"Time\").between(\"14:00:00\", \"16:59:59\")), \"afternoon\")\n",
    "                .otherwise(\"evening\"))\n",
    "\n",
    "item_counts = df3.groupBy(\"Daypart\", \"Item\").agg(count(\"Transaction\").alias(\"qty\"))\n",
    "top_items = item_counts.withColumn(\"rank\", rank().over(Window.partitionBy(\"Daypart\").orderBy(col(\"qty\").desc()))).filter(col(\"rank\") <= 3)\n",
    "top_items = top_items.groupBy(\"Daypart\").agg(concat_ws(\", \", collect_list(\"Item\")).alias(\"Top-3 Items\"))\n",
    "\n",
    "print(\"Top-3 (by qty) items bought by Daypart:\")\n",
    "top_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc3db4-16f1-4d57-a515-e9090bf9bd08",
   "metadata": {},
   "source": [
    "# Question 4: Duplicate Detection with Minhash – 50 points\n",
    "\n",
    "Minhash/LSH is an algorithm based on cryptographic hashes that computes similarity between a pair of entities. It is heavily used in Web Search and product similarity applications.\n",
    "\n",
    "Details of the algorithm can be found in Chapter 3 of Mining Massive Datasets.    \n",
    "http://infolab.stanford.edu/~ullman/mmds/ch3.pdf\n",
    "\n",
    "For example, assume I teach a course at NYU and I trust the students to learn the material by doing original work. However, I think I see too many cases of plagiarism and copying. I could use this algorithm to easily detect near-duplicates and copying so you I can fail those students. \n",
    "\n",
    "__Problem Statement__\n",
    "\n",
    "For this problem, let’s use the similarity algorithm to find the most similar items. Assume I you are reading news items in the HuffPost website (https://www.huffpost.com/) and last clicked on a new article about gun control in the USA. The next day, I return to the website and it decides to prioritize new items for me based on the last news item I read. \n",
    "\n",
    "__The Data__:\n",
    "\n",
    "In Jupyterhub: `shared/data/huffpost.json`\n",
    "Base news item short description \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    "\n",
    "__TODO__\n",
    "\n",
    "Use the Minhash/LSH algorithm to find URL link, headline, category, and short description of the 5 most similar items to the Item above (based on the \"short_description\" field). The algorithm relies on the concept of distances to define similarity. For this exercise, use Jaccard similarity. \n",
    "\n",
    "NOTE: You can use any a third party Python library like 'datasketch'. (http://ekzhu.com/datasketch/minhash.html). Datasketch is installed in the JupyterHub environment for this class. Note, SparkML also has a Minhash/LSH library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05c33c9-576d-438e-aa86-eba31e83e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasketch in /opt/conda/envs/bigdata-spark/lib/python3.11/site-packages (1.6.5)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/envs/bigdata-spark/lib/python3.11/site-packages (from datasketch) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/envs/bigdata-spark/lib/python3.11/site-packages (from datasketch) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d412913b-677f-4cf3-803f-f4d268bcbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1264995-ef3e-4f34-a7e2-0d2162f856ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minhash(short_desc):\n",
    "    m = MinHash()\n",
    "    for word in short_desc.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2b249b-58da-4e75-8d11-a44853ac33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(minhash1, minhash2):\n",
    "    return minhash1.jaccard(minhash2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c92be60e-9ae2-4cb0-abb6-58af0244e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- short_description: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.json('shared/data/Huffpost.json')\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a1fdac-d566-4718-b816-37a1dbbb2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash_udf = udf(lambda desc: get_minhash(desc), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142c17ad-a93b-4795-a692-b28087cc0ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- short_description: string (nullable = true)\n",
      " |-- minhash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df4.withColumn(\"minhash\", minhash_udf(col(\"short_description\")))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91164a3f-7fb2-4aa3-ac09-61019c2f38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_short_desc = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    "base_minhash = get_minhash(base_short_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b90ea579-db89-46eb-8fe2-ee4edba09b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|link                                                                                                                               |headline                                                                            |category |short_description                                                                                                              |Estimated Jaccard|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|https://www.huffingtonpost.com/entry/snl-sketch-for-liberal-wonderland-the-bubble-is-necessarily-honest_us_58331b15e4b058ce7aac1a6c|'SNL' Sketch For Liberal Wonderland 'The Bubble' Is ... Necessarily Honest          |COMEDY   |A community of like-minded freethinkers. And no one else.                                                                      |0.1328125        |\n",
      "|https://www.huffingtonpost.com/entry/a-sandy-hook-mom-on_us_5b9cae9fe4b03a1dcc80d7c2                                               |A Sandy Hook Mom On Sending Her Kids Back To School: 'I'm Hoping For A Peaceful Day'|PARENTING|40 Must-See Places to Take Your Kids Before They're Grown Exclusive: A Connecticut Mom Shares Her Terror: \"It Wasn't A Shooting|0.125            |\n",
      "|https://www.huffingtonpost.com/entry/matt-gaetz-flipped-off_us_5ae91a54e4b022f71a02f19a                                            |Trump-Loving Congressman Gets Flipped Off in Middle School Girl's Photobomb         |POLITICS |Apparently, Mom wasn't amused.                                                                                                 |0.109375         |\n",
      "|https://www.huffingtonpost.com/entry/boyfriend-rules-cute-kid-note_us_5b9dd3fde4b03a1dcc8d6f29                                     |30 Rules For Boyfriends From Two Wise Little Girls                                  |PARENTING|Authors: Blaire (age 6) and Brooke (age 9) Who is this mystery man? Mom says Blaire is \"absolutely boy crazy\" and likes to     |0.109375         |\n",
      "|https://www.huffingtonpost.com/entry/jimmy-fallon-myfirstapartment-tweets-hashtag_us_560e7553e4b0af3706e052f6                      |Jimmy Fallon's #MyFirstApartment Tweets Prove You Should Never Leave Home           |COMEDY   |Mom and Dad, we're baaaaacccckkkkkk!                                                                                           |0.109375         |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jaccard_udf = udf(lambda minhash: compute_jaccard(base_minhash, minhash), FloatType())\n",
    "\n",
    "df4 = df4.withColumn(\"Estimated Jaccard\", jaccard_udf(col(\"minhash\")))\n",
    "df4 = df4.orderBy(col(\"Estimated Jaccard\").desc()).limit(5)\n",
    "df4.select(\"link\", \"headline\", \"category\", \"short_description\", \"Estimated Jaccard\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8ef4f-2fba-475f-a8c1-10233a092b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
